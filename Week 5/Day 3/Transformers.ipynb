{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q&A**"
      ],
      "metadata": {
        "id": "QTWDG0SPArkI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8J7LwoK-2NY",
        "outputId": "d2c83b9f-3bdf-43ca-8a6f-a5b37f4de64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "qa = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "result = qa({\n",
        "    \"question\": \"What are transformers used for?\",\n",
        "    \"context\": \"Transformers are state-of-the-art models for NLP tasks.\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Answer:\", result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlTVKeQfAefT",
        "outputId": "1a8789a1-8df4-40b6-fbc8-a8bdbc302eab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: NLP tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "IAdpKgeiAyBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentiment = pipeline(\"sentiment-analysis\")\n",
        "result = sentiment(\"I absolutely love this movie!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcY3ZKI8_kuX",
        "outputId": "4261fa63-b890-4ea3-a83d-3c2d0910bc1e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Label: {result[0]['label']}, Score: {result[0]['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFpZ_z8fAnOz",
        "outputId": "751e5af2-25fb-444c-cb53-a80484dadeb3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: POSITIVE, Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name Entity Recognition**"
      ],
      "metadata": {
        "id": "X7HOgFR_A80I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "text = \"Barack Obama was the 44th president of the United States.\"\n",
        "results = ner(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gj9kFFD_pAb",
        "outputId": "5ad3a0d0-37a5-48a6-c88b-b473dda47e1c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in results:\n",
        "    print(f\"{entity['word']} -> {entity['entity_group']} ({entity['score']:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hidv7p7_Ak1h",
        "outputId": "4b91a6fb-71b4-4d49-ae09-033289f8cf5b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama -> PER (1.00)\n",
            "United States -> LOC (1.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**"
      ],
      "metadata": {
        "id": "er1L3E3NBHa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "text = generator(\"Once upon a time\", max_new_tokens=50, temperature=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O654a41_rtv",
        "outputId": "ddb907b6-6d9a-4d5f-902d-ce25cf611500"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated:\", text[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJ7E9rCBrjx",
        "outputId": "77e9a8a7-ed7d-4a1a-b04f-0ec0bdae6614"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: Once upon a time, in order to keep up with the increasing number of new players who are joining the series, the team simply had to add a few more players to the roster.\n",
            "\n",
            "\"We've got to be really clear about the fact that we're not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Summarization**"
      ],
      "metadata": {
        "id": "yslxn0KXBU0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "article = \"\"\"The Transformers architecture has been adopted as a general-purpose model that can be fine-tuned for many NLP tasks. It uses self-attention and positional embeddings to process sequences.\"\"\"\n",
        "summary = summarizer(article, max_length=50, min_length=10, do_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biwRf8h4_uH3",
        "outputId": "2c9cc385-d7d1-45e0-bbab-32a4496c5156"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 50, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary:\", summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R_BpDMEBOCE",
        "outputId": "f76edd3c-7f28-4ef3-d4f9-8dfef5a35ea5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  Transformers architecture has been adopted as a general-purpose model that can be fine-tuned for many NLP tasks . It uses self-attention and positional embeddings to process sequences .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BADYVPeqUJvp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}